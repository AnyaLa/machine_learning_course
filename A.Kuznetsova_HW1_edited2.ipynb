{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация новостных текстов #\n",
    "\n",
    "** Команда: **\n",
    "* Анна Лапидус\n",
    "* Анастасия Кузнецова\n",
    "* Надежда Катричева\n",
    "* Альфия Бабий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи и формулировка задания ##\n",
    "\n",
    "Итак в нашем распоряжении есть архив из новостей, которые соотнесены с одним из 28 резонансных событий. Эти события обозначены определенным номером (соответствует номеру кластера). В наши *задачи* входит:\n",
    "1. Провести кластеризацию текстов новостей и попытаться соотнести полученные в ходе анализа кластеры с данными кластерами;\n",
    "2. Провести кластеризацию на меньшее количество кластеров и проверить, получается ли выделить общие направления новостей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание ##\n",
    "1. Провести предобработку текста:\n",
    "    * Токенизация\n",
    "    * Приведение к нижнему регистру\n",
    "    * Лемматизация \n",
    "    * Найти дубликаты (если есть)\n",
    "    * Определить, сколько новостей относятся к каждому событию. \n",
    "2. С помощью алгоритма **K-means** разбить тексты на 28 кластеров и определить эффективность разбиения с помощью мер качества (Homogeneity, Completeness, V-measure, Adjusted Rand-Index).\n",
    "3. Определить, как разные способы обработки данных влияют на качество кластеризации. Мы будем использовать:\n",
    "    * TF-IDF преобразование\n",
    "    * Сингулярное разложение\n",
    "    * Нормировку признакового пространства. \n",
    "4. С помощью K-means повторно разбить новости на 5 кластеров и попытаться соотнести их с резонансными событиями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теоречиеское обоснование ##\n",
    "\n",
    "**K-means**\n",
    "\n",
    "В этом небольшом исследовании мы решили воспользоваться алгоритмомо к-средних, метрикой которого является евклидово расстояние. Его цель -- минимизировать расстояние между документами и кластерами (их центроидами). На первом этапе алгоритма выбираются случайным образом центры кластеров, а затем на кажом последующем шаге алгоритм стремится минимизировать расстояние между документами, изменяя координаты центроидов. При этом тексты представлены в виде векторов, и основной величиной, с помощью которой измеряется расстояние между центроидами является *остаточная сумма квадратов*. \n",
    "\n",
    "Мы решили воспользоваться этим алгоритмом, поскольку он оказывается довольно простым в реализации, но в то же время он довольно эффективен, в отличие, например, от алгоритма ближайших соседей, который показывает меньшую точность.\n",
    "\n",
    "Для увеличения эффективности алгоритма мы будем использовать следующие методы:\n",
    "\n",
    "**Сингулярное разложение**\n",
    "\n",
    "Сингулярное разложение необходимо для того, чтобы сократить размерность пространства признаков (слов), которыми мы характеризуем каждый документ.\n",
    "\n",
    "В резульатте разложения получается три матрицы, перемноженных между собой, в матрице по середине элементы только на диагонали (сингулярные числа) и их столько, сколько слов в нашем словаре. Мы оставляем заданное число этих сингулярных чисел (n компонент, которые задаются в методе), остальные обнуляем в результате при обратном перемножении размерность пространства признаков (количество слов) сократится дод заданного *n*. Это может улучшить качество кластеризации, поскольку мы избавляемся от ненужных признаков и оставляем только значимые.\n",
    "\n",
    "**Нормализация**\n",
    "\n",
    "Нормализация необходима для того, чтобы привести вектора к общей размерности. Не понятно, может ли это в нашем случае привести к значительному улучшению качества кластеризации, это мы орпеделим в ходе анализа.\n",
    "\n",
    "**TF-IDF**\n",
    "\n",
    "TF-IDF мера помогает нормировать текст, определяя наиболее важные слова (с большим весом) и отсеивая слишком частотные или очень редкие слова. Эта мера также может помочь нам существенно улучшить качество кластеризации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 1. Предобработка текстов #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импортируем библиотеки ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_rus = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Читаем csv из файла, создаем дата-фрейм с помощью pandas ##\n",
    "\n",
    "Всего в нашем дата-фрейме оказалось 1930 текстов. Посмотрим, сколько удастся выявить дубликатов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1930, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('raw_news.csv', index_col = 0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем список уникальных новостей (удаление дубликатов) ##\n",
    "\n",
    "Получаем 1924 уникальных новости и 6 дубликатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1924, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique = df.drop_duplicates(subset = 'text', keep = 'first')\n",
    "df_unique.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы было удобнее работать с данными, переведем их в обычный список. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news = df_unique['text'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google Новости ТОП, Москва, 14 января 2017 АКЦИЯ ПРОТИВ ПЕРЕДАЧИ ИСААКИЕВСКОГО СОБОРА РПЦ ПРОШЛА БЕЗ НАРУШЕНИЙ Москва. 13 января. INTERFAX.RU - Акция противников передачи Исаакиевского собора Русской православной церкви прошла без нарушений, сообщили \"Интерфаксу\" в пятницу в пресс-службе управления МВД по Санкт-Петербургу и Ленинградской области. \"Общественная акция рядом... http://www.interfax.ru/russia/545283#googletop\n"
     ]
    }
   ],
   "source": [
    "print(news[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверим сколько текстов есть для каждого резонансного события ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_id\n",
       "1     100\n",
       "2      51\n",
       "3      84\n",
       "4      62\n",
       "5       2\n",
       "6      41\n",
       "7     100\n",
       "8      27\n",
       "9      76\n",
       "10    100\n",
       "11     49\n",
       "12    100\n",
       "13     24\n",
       "14      2\n",
       "15      7\n",
       "16    100\n",
       "17    102\n",
       "18    100\n",
       "19     45\n",
       "20      8\n",
       "21    100\n",
       "22     82\n",
       "23    100\n",
       "24     62\n",
       "25    100\n",
       "26    100\n",
       "27    100\n",
       "28    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.groupby('event_id').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация текстов ##\n",
    "\n",
    "Проводим токенизацию в несколько этапов. Для этого напишем функцию, которая приводит тексты к нижнему регистру, с помощью регулярного выражения удалим все гиперссылки, избавимся от ненужных знаков препинания. Мы знаем, что метод *findall* возвращает нам список, поэтому на выходе получим список токенов для одного текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_news(single_new):\n",
    "    lower_new = single_new.lower().replace(' - ', ' ').replace('. ', ' ')\n",
    "    clean_new = re.sub(r'http\\S+', '', lower_new)\n",
    "    word_list = re.findall(r'[а-яa-z\\-\\.]+', clean_new)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['риа', 'новости', 'все', 'новости', 'закрытая', 'лента', 'москва', 'января', 'противники', 'передачи', 'исаакиевского', 'собора', 'рпц', 'намерены', 'выйти', 'на', 'митинг', 'января', 'с.-петербург', 'янв', 'риа', 'новости', 'активисты', 'выступающие', 'против', 'передачи', 'исаакиевского', 'собора', 'русской', 'православной', 'церкви', 'а', 'также', 'депутаты', 'законодательного', 'собрания', 'петербурга', 'в', 'пятницу', 'в', 'ходе', 'встречи', 'на', 'исаакиевской', 'площади', 'возле', 'собора', 'договорились', 'подать', 'заявку', 'на', 'проведение', 'митинга', 'января', 'сообщил', 'риа', 'новости', 'член', 'городского', 'парламента', 'борис', 'вишневский', 'заявка', 'на', 'проведение', 'митинга', 'на', 'марсовом', 'поле', 'января', 'уже', 'готова', 'мы', 'намерены', 'подать', 'ее', 'в', 'понедельник', 'сказал', 'он', 'во', 'вторник', 'пресс-секретарь', 'губернатора', 'санкт-петербурга', 'андрей', 'кибитов', 'сообщил', 'что', 'исаакиевский', 'собор', 'будет', 'передан', 'рпц', 'в', 'безвозмездное', 'пользование', 'однако', 'сохранит', 'музейно-просветительские', 'функции', 'позже', 'власти', 'петербурга', 'уточнили', 'что', 'собор', 'передается', 'рпц', 'в', 'безвозмездное', 'пользование', 'на', 'лет', 'без', 'смены', 'юридического', 'статуса', 'его', 'владельцем', 'остается', 'город', 'представители', 'рпц', 'заявили', 'что', 'музейные', 'функции', 'собора', 'будут', 'расширены', 'а', 'вход', 'в', 'собор', 'станет', 'бесплатным', 'тем', 'не', 'менее', 'общественность', 'обеспокоена', 'судьбой', 'исаакиевского', 'собор', 'а', 'как', 'передавал', 'ранее', 'корреспондент', 'риа', 'новости', 'на', 'встрече', 'перед', 'собравшимися', 'выступили', 'депутаты', 'законодательного', 'собрания', 'петербурга', 'вишневский', 'максим', 'резник', 'и', 'алексей', 'ковалев', 'на', 'исаакиевской', 'площади', 'наблюдалось', 'большое', 'количество', 'представителей', 'сми', 'здесь', 'же', 'находились', 'наряды', 'полиции', 'которые', 'не', 'препятствовали', 'проведению', 'встречи', 'патрулируя', 'территорию', 'как', 'сообщал', 'вишневский', 'на', 'своей', 'странице', 'в', 'facebook', 'поскольку', 'встреча', 'у', 'исаакия', 'имеет', 'формат', 'общения', 'парламентариев', 'с', 'избирателями', 'она', 'не', 'требует', 'дополнительных', 'согласований', 'с', 'городскими', 'властями', 'в', 'году', 'когда', 'городские', 'власти', 'отказали', 'санкт-петербургской', 'епархии', 'рпц', 'в', 'передаче', 'храма', 'в', 'безвозмездное', 'пользование', 'в', 'интернете', 'появилась', 'петиция', 'против', 'этой', 'инициативы', 'и', 'за', 'период', 'со', 'вторника', 'по', 'настоящее', 'время', 'ее', 'подписали', 'около', 'тысяч', 'человек', 'всего', 'петиция', 'на', 'сегодняшний', 'день', 'набрала', 'более', 'тысяч', 'голосов', 'также', 'группа', 'петербургских', 'активистов', 'запустила', 'специальный', 'сайт', 'за', 'сохранение', 'статуса', 'исаакиевского', 'собора', 'как', 'музея-памятника', 'и', 'против', 'его', 'передачи', 'в', 'пользование', 'рпц', 'на', 'этом', 'сайте', 'также', 'осуществляется', 'сбор', 'подписе', 'которые', 'инициаторы', 'кампании', 'обещают', 'перенаправить', 'в', 'администрацию', 'петербурга', 'в', 'виде', 'официального', 'обращения', 'гражданина', 'в', 'органы', 'власти', 'исаакиевский', 'собор', 'был', 'построен', 'в', '-', 'годах', 'по', 'проекту', 'архитектора', 'огюста', 'монферрана', 'его', 'высота', 'превышает', 'метров', 'а', 'внутренняя', 'площадь', 'более', 'тысяч', 'квадратных', 'метров', 'правительство', 'рф', 'передало', 'исаакиевский', 'собор', 'в', 'собственность', 'санкт-петербурга', 'в', 'октябре', 'года', 'тремя', 'годами', 'позже', 'санкт-петербургская', 'епархия', 'рпц', 'обратилась', 'к', 'городским', 'властям', 'с', 'просьбой', 'передать', 'ей', 'здание', 'исаакиевского', 'собора', 'в', 'безвозмездное', 'пользование', 'но', 'получила', 'отказ', 'правительство', 'города', 'тогда', 'приняло', 'решение', 'о', 'сохранении', 'исаакиевского', 'собора', 'в', 'о', 'еративном', 'управлении', 'государственного', 'музея-памятника', 'исаакиевский', 'собор', '.']\n"
     ]
    }
   ],
   "source": [
    "print(clean_news(news[30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация с помощью Pymorphy2 ##\n",
    "\n",
    "Для лемматизации нам понадобится еще один модуль, поэтому импортируем его. Затем пишем функцию, которая возвращает нам строку из лемм, создаем список лемм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "аргумент и факт aif.ru москва январь оппозиция провести митинг против передача исаакиевский собор рпц протестный мероприятие собрать около тысяча человек москва январь аиф-москва депутат законодательный собрание санкт-петербург от фракция яблоко борис вишневский сообщить что январь перед исаакиевский собор быть провести митинг протест против передача собор русский православный церковь передавать рбк сообщаться что мероприятие стать один в серия протестный акция участник мероприятие стать около тысяча человек кроме вишневский организатор выступить депутат заксобрание максим резник партия рост и алексей ковалев справедливый россия также депутат добавить что в настоящее время готовиться иск о оспаривание передача собор так как речь идти о грубый нарушение закон о передача религиозный организация имущество религиозный назначение находиться в государственный или муниципальный собственность ранее сообщаться что собор быть передать в безвозмездный пользование рпц по просьба патриарх кирилл два тысяча человек выступить против передача исаакиевский собор рпц ранее рпц уже отказывать в просьба передать собор ivan smelov \n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer() \n",
    "cache = dict() # ускоряем разбор слов, избавившись от повторной работы\n",
    "\n",
    "def lemmatizer(token_list):\n",
    "    lemmas = ''\n",
    "    for word in token_list:\n",
    "        if word in cache:\n",
    "            lemma = cache[word]\n",
    "        else:\n",
    "            lemma = morph.parse(word)[0].normal_form\n",
    "            cache[word] = lemma\n",
    "        lemmas += lemma + ' '\n",
    "    return lemmas\n",
    "word_list = clean_news(news[2])\n",
    "print(lemmatizer(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку CountVectorizer из модуля sklearn принимает на вход списки, мы создаем список лематизированных текстов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemma_list = []\n",
    "\n",
    "for item in news:\n",
    "    tokens = clean_news(item)\n",
    "   # token_list.append(tokens)\n",
    "    lemmas = lemmatizer(tokens)\n",
    "    lemma_list.append(lemmas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечем все event_id  для того, чтобы соотнести их со списками лемм ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 ..., 28 28 28]\n"
     ]
    }
   ],
   "source": [
    "event_ids = df_unique['event_id'].values\n",
    "print(event_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = list(zip(event_ids, lemma_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   event_id                                             lemmas\n",
      "0         1  в петербург пройти митинг против передача исаа...\n",
      "1         1  lenta.co москва январь ситуация с передача иса...\n",
      "2         1  аргумент и факт aif.ru москва январь оппозиция...\n",
      "3         1  google новость топ москва январь акция против ...\n"
     ]
    }
   ],
   "source": [
    "df_lemma = pd.DataFrame(data = dataset, columns = ['event_id', 'lemmas'])\n",
    "print(df_lemma[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 2. Эксперименты#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация K-means ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним кластеризацию _k-means_ на 28 кластеров без использования текстовых преобразований нормалайзера и TF-IDF, а в последующих шагах посмотрим, как изменилась точность кластеризации после применения каждого метода. Для `min_df`, `max_df`, `ngram_range` и `analyzer` мы выбрали те параметры, при которых удалось получить наилучшие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запустим кластеризацию без дополнительных параметров ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['и', 'в', ...=28, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df = 0.9, min_df = 3, stop_words = stopwords_rus, analyzer = 'word')),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    #('svd', TruncatedSVD(n_components = 1500)),\n",
    "    #('norm', Normalizer() ),\n",
    "    ('clust', KMeans(n_clusters = 28, random_state = 42))\n",
    "])\n",
    "\n",
    "# pipeline.fit(df_unique['text'].values)\n",
    "pipeline.fit(df_lemma['lemmas'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы зафиксировали random state для того, чтобы точнее сравнивать точность методов между собой: \n",
    "\n",
    "**` KMeans(n_clusters = 28, random_state = 42)`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13601\n"
     ]
    }
   ],
   "source": [
    "print(len(pipeline.named_steps['vect'].vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.299238061546\n",
      "Completeness: 0.59569106146\n",
      "V-measure 0.398363253422\n",
      "Adjusted Rand-Index: 0.0507037962299\n"
     ]
    }
   ],
   "source": [
    "clust_labels = pipeline.named_steps['clust'].labels_\n",
    "labels = df_lemma['event_id']\n",
    "\n",
    "print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Не очень =( ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем сингулярное разложение ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['и', 'в', ...=28, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df = 0.9, min_df = 3, stop_words = stopwords_rus, analyzer = 'word')),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD(n_components = 1500)),\n",
    "    #('norm', Normalizer() ),\n",
    "    ('clust', KMeans(n_clusters = 28, random_state = 42))\n",
    "])\n",
    "\n",
    "pipeline.fit(df_lemma['lemmas'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.236509918002\n",
      "Completeness: 0.572003668854\n",
      "V-measure 0.334650012113\n",
      "Adjusted Rand-Index: 0.023971397177\n"
     ]
    }
   ],
   "source": [
    "clust_labels = pipeline.named_steps['clust'].labels_\n",
    "labels = df_lemma['event_id']\n",
    "\n",
    "print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итак без использования TF-IDF и нормалайзера мы получили очень маленькую точность даже при использовании SVD.** Добавим TF-IDF преобразование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['и', 'в', ...=28, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df = 0.9, min_df = 3, stop_words = stopwords_rus, analyzer = 'word')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD(n_components = 1500)),\n",
    "    #('norm', Normalizer() ),\n",
    "    ('clust', KMeans(n_clusters = 28, random_state = 42))\n",
    "])\n",
    "\n",
    "pipeline.fit(df_lemma['lemmas'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.944437942137\n",
      "Completeness: 0.936065114609\n",
      "V-measure 0.940232888616\n",
      "Adjusted Rand-Index: 0.852724264627\n"
     ]
    }
   ],
   "source": [
    "clust_labels = pipeline.named_steps['clust'].labels_\n",
    "labels = df_lemma['event_id']\n",
    "\n",
    "print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**С TF-IDF преобразованием мы получили уже гораздо большую точность -- 85%. Уже гораздо лучше** Теперь добавим нормализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['и', 'в', ...=28, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df = 0.9, min_df = 3, stop_words = stopwords_rus, analyzer = 'word')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD(n_components = 1500)),\n",
    "    ('norm', Normalizer() ),\n",
    "    ('clust', KMeans(n_clusters = 28, random_state = 42))\n",
    "])\n",
    "\n",
    "# pipeline.fit(df_unique['text'].values)\n",
    "pipeline.fit(df_lemma['lemmas'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.944448544976\n",
      "Completeness: 0.93995242123\n",
      "V-measure 0.942195119295\n",
      "Adjusted Rand-Index: 0.83850548439\n"
     ]
    }
   ],
   "source": [
    "clust_labels = pipeline.named_steps['clust'].labels_\n",
    "labels = df_lemma['event_id']\n",
    "\n",
    "print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получаем точность 83%. Не сильно изменилась с предыдущего шага, даже слегка уменьшилась** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0, 100,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  43,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          8,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  84,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  62,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          2,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,  41,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0, 100,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         27,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  73,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          3,   0,   0],\n",
       "       [  0,   0,   0,  93,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   7,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,  49,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  99,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,  24,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   2,   0],\n",
       "       [  0,   0,   0,   6,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          1,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  65,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 100,   0,\n",
       "          1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,   0],\n",
       "       [  0,   0, 100,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  26,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  82,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,  68,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,  32,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         61,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   2,   0,   1,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  97,\n",
       "          0,   0,   0],\n",
       "       [ 23,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,\n",
       "          0,   0,   0,   0,  56,   0,   0,   0,   0,  20,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0, 100,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        100,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels, clust_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, максимальный Rand Index, который мы получили составил 85%. Чтобы оценить роль нормализации текста, попробуем проделать кластеризацию текста без предобработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=0.1,\n",
       "        ngram_range=[2, 3], preprocessor=None, stop_words=None,\n",
       "        ...8, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df = 0.9, min_df = 0.1, ngram_range = [2, 3], analyzer = 'char_wb')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD(n_components = 50)),\n",
    "    ('norm', Normalizer() ),\n",
    "    ('clust', KMeans(n_clusters = 28))\n",
    "])\n",
    "\n",
    "pipeline.fit(df_unique['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.898025624474\n",
      "Completeness: 0.881114598483\n",
      "V-measure 0.88948974041\n",
      "Adjusted Rand-Index: 0.757615603852\n"
     ]
    }
   ],
   "source": [
    "clust_labels = pipeline.named_steps['clust'].labels_\n",
    "labels = df_lemma['event_id']\n",
    "\n",
    "print(\"Homogeneity:\", homogeneity_score(labels, clust_labels))\n",
    "print(\"Completeness:\", completeness_score(labels, clust_labels))\n",
    "print(\"V-measure\",  v_measure_score(labels, clust_labels))\n",
    "print(\"Adjusted Rand-Index:\",  adjusted_rand_score(labels, clust_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как видим, предобработка текста добавила точности в 10%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 3. Общие направления новостей #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заимпортируем файл с классификацией резонансных событий. Для того, чтобы в последующих шагах соотнести новые кластеры со старыми событиями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-10 00:00:00</td>\n",
       "      <td>Власти Петербурга согласились передать РПЦ Иса...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-01-20 00:00:00</td>\n",
       "      <td>Дональд Трамп вступил в должность президента США.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-02-20 00:00:00</td>\n",
       "      <td>Скоропостижно скончался постпред России при ОО...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-03-02 00:00:00</td>\n",
       "      <td>Вышел фильм Навального «он Вам не димон»</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2017-03-14 00:00:00</td>\n",
       "      <td>CNN показала фильм «Владимир Путин — самый вли...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 date                                               name\n",
       "0   1  2017-01-10 00:00:00  Власти Петербурга согласились передать РПЦ Иса...\n",
       "1   2  2017-01-20 00:00:00  Дональд Трамп вступил в должность президента США.\n",
       "2   3  2017-02-20 00:00:00  Скоропостижно скончался постпред России при ОО...\n",
       "3   4  2017-03-02 00:00:00           Вышел фильм Навального «он Вам не димон»\n",
       "4   5  2017-03-14 00:00:00  CNN показала фильм «Владимир Путин — самый вли..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = pd.read_csv('events.csv', sep = ',', header = 0)\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['и', 'в', ...s=7, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2 = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df = 0.9, min_df = 3, stop_words = stopwords_rus, analyzer = 'word')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD(n_components = 1500)),\n",
    "    ('norm', Normalizer() ),\n",
    "    ('clust', KMeans(n_clusters = 7, random_state = 42))\n",
    "])\n",
    "\n",
    "pipeline2.fit(df_lemma['lemmas'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   name                                                                                                    \n",
       "0  Акции протеста 12 июня                                                                                      100\n",
       "   Власти Петербурга согласились передать РПЦ Исаакиевский собор.                                              100\n",
       "   Митинг в москве против коррупции                                                                            100\n",
       "   В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков                                   73\n",
       "   Вышел фильм Навального «он Вам не димон»                                                                     62\n",
       "   Митинг против Реновации в Москве                                                                              8\n",
       "   Несанкционированные акции в Москве апрель                                                                     6\n",
       "   Дональд Трамп вступил в должность президента США.                                                             5\n",
       "   Премьер Медведев выступает перед депутатами Госдумы с отчетом об итогах работы правительства за 2016 год      2\n",
       "1  Единый день голосования                                                                                      99\n",
       "   Парламентские выборы в Великобритании                                                                        82\n",
       "   Чемпионат мира по хоккею                                                                                      1\n",
       "2  Чемпионат мира по хоккею                                                                                    101\n",
       "   Кубок конфедерации FiFA                                                                                     100\n",
       "   SpaceX впервые в истории запустила и посадила уже летавшую ракету-носитель                                   49\n",
       "   Победа Макрона во Франции                                                                                    45\n",
       "   Юлию Самойлову не пустили на евровидении в Киеве                                                             27\n",
       "   Дональд Трамп вступил в должность президента США.                                                             3\n",
       "   Единый день голосования                                                                                       1\n",
       "   Умер Евгений Евтушенко                                                                                        1\n",
       "3  Умер Евгений Евтушенко                                                                                       99\n",
       "   Скоропостижно скончался постпред России при ООН Виталий Чуркин.                                              84\n",
       "   Умер Дэвид рокфеллер                                                                                         41\n",
       "4  Правительство внесло в Госдуму законопроект о курортных сборах                                              100\n",
       "5  Путин и Меркель в Сочи                                                                                      100\n",
       "   Саммит G20                                                                                                  100\n",
       "   Горячая линия Президента Путина                                                                              62\n",
       "   Дональд Трамп вступил в должность президента США.                                                            43\n",
       "   Тиллерсон посещает Москву и встречается с Путиным                                                            24\n",
       "   В центре Киева был убит бывший депутат Госдумы РФ от КПРФ Денис Вороненков                                    3\n",
       "   CNN показала фильм «Владимир Путин — самый влиятельный человек в мире».                                       2\n",
       "   Несанкционированные акции в Москве апрель                                                                     1\n",
       "6  Теракт в Барселоне                                                                                          100\n",
       "   Ураган в Москве                                                                                             100\n",
       "   теракт произошел в центре Лондона                                                                           100\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_labels2 = pipeline2.named_steps['clust'].labels_\n",
    "text_labels = events.name.loc[labels-1]\n",
    "text_labels.groupby(clust_labels2).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Интерпретация результатов ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате кластеризации на 7 кластеров методом KMeans получены основные направления новостей:\n",
    "    \n",
    "* 0 - Акции протеста, политика\n",
    "* 1 - Выборы, голосования\n",
    "* 2 - Спорт, соревнования\n",
    "* 3 - Смерть\n",
    "* 4 - Законопроект\n",
    "* 5 - Путин, президенты\n",
    "* 6 - Теракты, ураганы\n",
    "    \n",
    "Есть единичные случаи попадания событий в несоответствующие кластеры, но, в основном, события, попавшие в один кластер, действительно объединены общей темой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
